import json
from typing import Dict
from langchain.schema import HumanMessage
from langgraph.graph import StateGraph, END
from .search_relevant_laws import retrieve_relevant_laws
from models.schemas import State



def build_graph(llm):
    def user_input_node(state: State):
        try:
            last_msg = state["messages"][-1]
            if isinstance(last_msg, dict):
                query = last_msg.get("content", "")
            else:
                query = getattr(last_msg, "content", "")

            print(f"\nüß† User query: {query}")

            # Wrap prompt in HumanMessage
            prompt = f"""X√°c ƒë·ªãnh xem c√¢u h·ªèi n√†y c√≥ c·∫ßn tra c·ª©u th√¥ng tin t·ª´ t√†i li·ªáu lu·∫≠t c·ªßa Vi·ªát Nam kh√¥ng:
            C√¢u h·ªèi: {query}

            Ch·ªâ tr·∫£ l·ªùi 'legal' ho·∫∑c 'general':"""

            is_legal_query = llm.invoke([HumanMessage(content=prompt)]).content.lower().strip()

            route = "legal_search" if "legal" in is_legal_query else "direct_answer"
            print(f"Query type: {'C·∫ßn tra c·ª©u t√†i li·ªáu' if route == 'legal_search' else 'Ki·∫øn th·ª©c chung'}")

            return {
                "query": query,
                "messages": state["messages"],
                "route": route
            }
        except Exception as e:
            print(f"Error in user_input_node: {e}")
            return {"query": "", "messages": state["messages"], "error": str(e)}

    def direct_answer_node(state: State):
        try:
            query = state.get("query", "")
            prompt = f"""B·∫°n l√† tr·ª£ l√Ω h·ªØu √≠ch tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ lu·∫≠t b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n c·ªßa Vi·ªát Nam.
            Tr·∫£ l·ªùi c√¢u h·ªèi n√†y ch·ªâ s·ª≠ d·ª•ng ki·∫øn th·ª©c chung c·ªßa b·∫°n:

            C√¢u h·ªèi: {query}

            Tr·∫£ l·ªùi ng·∫Øn g·ªçn kh√¥ng tr√¨nh b√†y d√†i d√≤ng."""

            answer = llm.invoke(prompt).content
            #print(f"\nüìù Direct Answer:\n{answer}")
            return {"messages": [{"role": "assistant", "content": answer}]}
        except Exception as e:
            print(f"Error in direct_answer_node: {e}")
            return {"messages": [{"role": "assistant", "content": f"T√¥i g·∫∑p l·ªói: {e}"}]}

    def refine_query_node(state: State):
        try:
            query = state.get("query", "")
            prompt = f"""N√¢ng cao c√¢u truy v·∫•n n√†y ƒë·ªÉ t√¨m ki·∫øm t·ªëi ∆∞u trong vƒÉn b·∫£n lu·∫≠t c·ªßa Vi·ªát Nam.

            C√¢u truy v·∫•n g·ªëc: {query}

            H√£y:
            1. S·ª≠a l·ªói ch√≠nh t·∫£ v√† thu·∫≠t ng·ªØ ph√°p l√Ω (n·∫øu c√≥)
            2. Chuy·ªÉn ƒë·ªïi c√¢u h·ªèi th√¥ng th∆∞·ªùng th√†nh c√¢u truy v·∫•n ph√°p l√Ω ch√≠nh x√°c
            3. Th√™m c√°c thu·∫≠t ng·ªØ chuy√™n ng√†nh li√™n quan ƒë·∫øn ph√°p lu·∫≠t
            4. X√°c ƒë·ªãnh c√°c ƒëi·ªÅu kho·∫£n ho·∫∑c kh√°i ni·ªám ph√°p l√Ω c·ª• th·ªÉ (n·∫øu c√≥ th·ªÉ)
            5. Gi·ªØ ng·∫Øn g·ªçn, s√∫c t√≠ch v√† r√µ r√†ng v·ªÅ √Ω ƒë·ªãnh t√¨m ki·∫øm

            V√≠ d·ª•:
            - "quy·ªÅn c·ªßa ng∆∞·ªùi d√πng khi b·ªã l·ªô th√¥ng tin" ‚Üí "quy·ªÅn c·ªßa ch·ªß th·ªÉ d·ªØ li·ªáu c√° nh√¢n khi x·∫£y ra vi ph·∫°m d·ªØ li·ªáu"
            - "ai qu·∫£n l√Ω d·ªØ li·ªáu c√° nh√¢n" ‚Üí "b√™n ki·ªÉm so√°t d·ªØ li·ªáu c√° nh√¢n v√† tr√°ch nhi·ªám ph√°p l√Ω"

            C√¢u truy v·∫•n ƒë√£ c·∫£i thi·ªán:"""


            refined = llm.invoke(prompt).content
            # print(f"üîç Refined Query: {refined}")

            # Update the state directly
            state["refined_query"] = refined
            return state # Return the modified state
        except Exception as e:
            print(f"Error in refine_query_node: {e}")
            state["refined_query"] = state.get("query", "") # Fallback to original query
            state["error"] = str(e)
            return state

    def semantic_search_node(state: State):
        """
        Perform semantic search with error handling and relevance filtering.
        """
        try:
            query = state.get("refined_query") or state.get("query", "")
            # print(f"üîé Searching with query: {query}")

            top_docs = retrieve_relevant_laws(
                query=query,
                n_resources_to_return=3,
                print_time=True
            )
            for doc in top_docs:
                print(json.dumps(doc, ensure_ascii=False, indent=2))

            return {"top_docs": top_docs, "query": query, "messages": state["messages"]}
        except Exception as e:
            print(f"Error in semantic_search_node: {e}")
            return {"top_docs": [], "query": state.get("query", ""), "messages": state["messages"], "error": str(e)}

    def generate_answer_node(state: State, llm=llm):
        try:
            query = state.get("query", "")
            top_docs = state.get("top_docs", [])
            # print(f"query", query)
            # print(f"top_docs", top_docs)
            sentence_chunks = [doc["sentence_chunk"] for doc in top_docs]
            context = " ".join(sentence_chunks)
              # Add page references
            page_refs = [str(doc["page_number"]) for doc in top_docs]
            unique_refs = sorted(set(page_refs))
            refs_text = f"\n\n*Tham kh·∫£o: Trang {', '.join(unique_refs)} c·ªßa Lu·∫≠t B·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n.*"


            if not top_docs:
                return {"messages": [{"role": "assistant", "content": "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c."}]}

            # print(f"context", context)
            # print(f"üìö Context length: {len(context)} characters")

            prompt = f"""B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ lu·∫≠t c·ªßa Vi·ªát Nam.
            Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c tr√≠ch ƒëo·∫°n vƒÉn b·∫£n lu·∫≠t b√™n d∆∞·ªõi.

            C√°c tr√≠ch ƒëo·∫°n:
            {context}

            C√¢u h·ªèi: {query}

            Tr·∫£ l·ªùi theo c√°c ti√™u ch√≠ sau:
            1. D·ª±a tr√™n c√°c tr√≠ch ƒëo·∫°n vƒÉn b·∫£n lu·∫≠t
            2. N√™u r√µ ƒëi·ªÅu, kho·∫£n li√™n quan n·∫øu c√≥
            3. Gi·∫£i th√≠ch b·∫±ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n v√† d·ªÖ hi·ªÉu
            4. N·∫øu tr√≠ch ƒëo·∫°n kh√¥ng ƒë·ªß th√¥ng tin ho·∫∑c kh√¥ng c√≥ th√¥ng tin li√™n quan v·ªÅ c√¢u h·ªèi, h√£y th·ª´a nh·∫≠n ƒëi·ªÅu ƒë√≥ m·ªôt c√°ch trung th·ª±c
            5. N·∫øu b·∫°n c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi c√¢u h·ªèi h√£y th√™m {refs_text}
            """

            response = llm.invoke([HumanMessage(content=prompt)])

            # Get content from AIMessage response
            answer = response.content if hasattr(response, 'content') else str(response)

            # Add references if we have content
            if answer and top_docs:
                answer = f"{answer}\n{refs_text}"

            return {"messages": [{"role": "assistant", "content": answer}]}
        except Exception as e:
            print(f"Error in generate_answer_node: {e}")
            return {"messages": [{"role": "assistant", "content": f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi: {e}"}]}


    graph_builder = StateGraph(State)

    graph_builder.add_node("user_input", user_input_node)
    graph_builder.add_node("direct_answer", direct_answer_node)
    graph_builder.add_node("refine_query", refine_query_node)
    graph_builder.add_node("semantic_search", semantic_search_node)
    graph_builder.add_node("generate_answer", generate_answer_node)

    graph_builder.set_entry_point("user_input")
    graph_builder.add_conditional_edges(
        "user_input",
        lambda state: state.get("route", "legal_search"),
        {
            "direct_answer": "direct_answer",
            "legal_search": "refine_query"
        }
    )
    graph_builder.add_edge("refine_query", "semantic_search")
    graph_builder.add_edge("semantic_search", "generate_answer")
    graph_builder.add_edge("direct_answer", END)
    graph_builder.add_edge("generate_answer", END)
    graph = graph_builder.compile()

    return graph